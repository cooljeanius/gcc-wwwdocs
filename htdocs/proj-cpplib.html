<html>

<head>
<title>cpplib TODO</title>
</head>

<body>
<h1 align="center">Projects relating to cpplib</h1>

<p>As of 11 September 2000, cpplib has largely been completed.  It has
received six months of testing as the only preprocessor used by
development gcc, and I'm pretty happy with its stability at this point.

<p>cpplib is still not linked into the C and C++ front ends by
default, but the remaining issues are minor.  It would be nice if
integrated mode could be default - or even mandatory! - by GCC 3.0.

<h2>How to help test</h2>

<p>Testing is not really necessary, unless you are prepared to test
configurations with <code>--enable-c-cpplib</code>.  If you do this,
be prepared for odd glitches - see below for the list of known problems.

<p>The best thing to test with the integrated preprocessor is large
packages that (ab)use the preprocessor heavily.  The compiler itself
is pretty good at that, but doesn't cover all the bases.  If you've
got cycles to burn, please try one or more of:

<ul>
  <li>BSD 'make world'
  <li>Binutils
  <li>Emacs
  <li>GNOME
  <li>GNU libc
  <li>Guile
  <li>Linux kernel (esp. non-i386)
  <li>Mozilla
  <li>Obfuscated C Contest entries
  <li>Perl
  <li>X11
  <li>... and anything else you can think of.
</ul>

<p>A bug report saying 'package FOO won't compile on system BAR' is
useless.  We need short testcases with no system dependencies.  Aim
for less than fifty lines and no #includes at all.  I recognize this
won't always be possible.

<p>Also, please file off everything that would cause us legal trouble
if we were to roll your test case into the distributed test suite.
Short test cases will almost always fall under fair use guidelines, so
don't sweat it too much.  An example of a problem is if your test case
includes a 200-line comment detailing inner workings of your program.
(A 200-line comment might be what you need to provoke a bug, but its
contents are unlikely to matter.   Try running it through 
<code>"tr A-Za-z x"</code>.)

<p>As usual, report bugs to <a
href="mailto:gcc-bugs@gcc.gnu.org">gcc-bugs@gcc.gnu.org</a>.  But
please read the rest of this document first!

<p>Bug reports in code which must be compiled with <code>gcc
-traditional</code> are of interest, but much lower priority than
standard conforming C/C++.  Traditional mode is implemented by a
separate program, not by cpplib.

<h2>Known Bugs</h2>

<p><ol>
  <li>If the integrated preprocessor is used, the
      <code>-traditional</code>, <code>-g3</code>, and
      <code>save-temps</code> options do not work as documented.
      <code>-traditional</code> does not invoke the (separate)
      traditional preprocessor; <code>-g3</code> does not add
      information about macro definitions to the debugging output; and
      <code>-save-temps</code> does not generate a file of
      preprocessed text.

      <br>All these are relatively simple to fix.
      <code>-traditional</code> and <code>-save-temps</code> simply
      require someone to hack up the "specs" (found in
      <file>gcc.c</file> and <file>*/lang-specs.h</file>) so that if
      either is used we revert to the external preprocessor.

      <br>To implement <code>-g3</code>, someone must write glue
      functions to sit between cpplib's <code>define</code> and
      <code>undef</code> callbacks, and the debugging output modules.
      That someone should also flesh out DWARF 2's support for macros
      in debug information; it is presently only a stub.

  <li>Character sets that are <em>not</em> strict supersets of ASCII
      may cause cpplib to mangle the input file, even in comments or
      strings.  Unfortunately, that includes important character sets
      such as Shift JIS and UCS2.  (Please see the discussion of <a
      href="#charset">character set issues</a>, below.)

  <li>Massively parallel builds may cause problems if your system has
      a global limit on the number of files mapped into memory.  I am
      not aware of any system with this problem, it is purely
      theoretical.

  <li>We decided recently to make backslash, whitespace, newline a
      line continuation (with a warning).  This is almost always an
      editing mistake, and causes floods of errors if rejected.  This
      has been implemented as a ten-line hack and the semantics are
      not quite right.  It doesn't work in comments, and in running
      text a block comment can appear between the backslash and the
      newline, which is not intended.  However, since this is only an
      error-recovery issue, fixing it is not critical.
</ol>

<h2>Missing User-visible Features</h2>

<p><ol>
  <li>Character sets that are strict supersets of ASCII are safe to
      use, but extended characters cannot appear in identifiers.  This
      has to be coordinated with the C and C++ front ends.  See <a
      href="#charset">character set issues</a>, below.

  <li>C99 universal character escapes (<code>\uxxxx</code>,
      <code>\Uxxxxxxxx</code>) are not recognized except in string
      or character constants, and will be misinterpreted in character
      constants appearing in #if directives.  Again, proper support
      has to be coordinated with the compiler proper.

  <li>C99's <code>_Pragma</code> intrinsic is not supported.  This is
      straightforward to implement: <code>_Pragma</code> is a special
      symbol (see <code>special_symbol</code> in <file>cpplex.c</file>
      which parses its argument, destringizes it, and then calls
      <code>_cpp_run_directive</code> to forward it to the
      <code>#pragma</code> handler.

  <li>Precompiled headers are commonly requested; this entails the
      ability for cpp to dump out and reload all its internal state.
      You can get some of this with the debug switches, but not all,
      and not in a reloadable format.  The front end must cooperate
      also.
</ol>

<h2>Internal work that needs doing</h2>

<ol>
  <li>The macro expander has been rewritten, but the new
      implementation is extremely clever and could probably benefit
      from simplification.

  <li>The lexical analyzer has been rewritten to be one-pass.  Again,
      the new implementation is extremely clever and should be
      simplified.  Also, it has not been tuned, and is usually in the
      top ten in profiles.

  <li>We allocate lots of itty bitty items with malloc.  Some work has
      been done on aggregating these into big blocks, using obstacks,
      but we could do even more.  Again, this can be a performance issue.

  <li>VMS support has suffered extreme bit rot.  There may be problems
      with support for DOS, Windows, MVS, and other non-Unixy
      platforms.  I can fix none of these myself.
</ol>

<h2>Integrating cpplib with the C and C++ front ends</h2>

This is mostly done.

<ol>
  <li>Front ends need to use cpplib's line and column numbering
      interface directly.  The existing code copies cpplib's internal
      state into the state used by <file>diagnostic.c</file>, which is
      better than writing out and processing linemarker commands, but
      still suboptimal.

  <li>The identifier hash tables used by cpplib and the front end
      should be unified.  In breadboard tests, this can net up to 10%
      speedup, mainly because the hash table used by front ends now
      (see <file>tree.c</file>) is no good.

  <li>If Yacc did not insist on assigning its own values for token
      codes, there would be no need for a translation layer between
      the codes returned by cpplib and the codes used by the parser.
      Noises have been made about a recursive-descent parser that
      could handle all of C, C++, Objective C; if this ever happens,
      it should use cpplib's token codes.

  <li>The work currently done by <code>c-lex.c</code> converting
      constants of various stripes to their internal representations
      might be better off done in cpplib.  I can make a case either
      way.
</ol>

<h2>Optimizations</h2>

<ol>
  <li>At the moment, we cache file buffers in memory as they appear on
      disk.  It might be worthwhile to do lexical analysis over the
      entire file and cache it like that, before directive processing
      and macro expansion.  This would save a good deal of work for
      files that are included more than once.  However, it would be
      less efficient for files included only once due to increased
      memory requirements; how do we tell the difference?

  <li>A complement to the usual one-huge-file scheme of precompiled
      headers would be to cache files on disk after lexical analysis.
      You could run a cruncher over <code>/usr/include</code> and save
      the results in a <code>.jar</code> file or similar, bypassing
      filesystem overhead as well as the work of lexical analysis.

  <li>Wrapper headers - files containing only an #include of another
      file - should be optimized out on reinclusion.  (Just tweak the
      include-file table entry of the wrapper to point to the file it
      reads.)

  <li>When a macro is defined to itself, bypass the macro expander
      entirely.  (Partially implemented.)
</ol>

<h2><a name="charset">Character set issues</a></h2>

<p>Proper character set handling is a hard problem.  Users want to be
able to write comments and strings in their native language.  They
want the strings to come out in their native language and not
gibberish after translation to object code.  Some users also want to
use their own alphabet for identifiers in their code.  There is no
one-to-one or many-to-one map between languages and character sets.
The subset of ASCII that is included in most modern day character sets
does not include all the punctuation C uses; some of the missing
punctuation may be present but at a different place than where it is
in ASCII.  The subset described in ISO646 may not be the smallest
subset out there.

<p>At the present time, GCC supports the use of any character set in
comments and strings, as long as it is a strict superset of 7-bit
ASCII.  By this I mean that all printable (including whitespace) ASCII
characters, when they appear as single bytes in a file, stand only for
themselves, no matter what the context is.  This is true of ISO8859.x,
KOI8-R, and UTF8.  It is not true of Shift JIS and other popular Asian
character sets.  If you use the C99 <code>\u</code> and
<code>\U</code> escapes, you get UTF8, no exceptions.  These too are
only supported in string and character constants.  Non-ASCII
characters in strings are copied to the assembly output verbatim.

<p>We intend to improve this as follows:

<ol>
  <li>cpplib will be reworked so that it can handle any character set
      in wide use, whether or not it is a strict superset of 7-bit
      ASCII.  This means that cpplib will never confuse non-ASCII
      characters with C punctuators, comment delimiters, or whatever.

  <li>In comments, naturally any character will be permitted to appear.

  <li>All Unicode code points which are permitted by C99 Annex D to
      appear in identifiers, will be accepted in identifiers.  All
      source-file characters which, when translated to Unicode,
      correspond to permitted code points, will also be accepted.  In
      assembly output, identifiers will be encoded in UTF8, and then
      reencoded using some mangling scheme if the assembler cannot
      handle UTF8 identifiers.  (Does the new C++ ABI have anything to
      say about this?  What does the Java compiler do?)

      <br>Unicode <code>U+0024</code> will be permitted in
      identifiers if and only if <code>$</code> is permitted.

  <li>In strings and character constants, GCC will translate from the
      character set of the file (selectable on a per-file basis), to
      the current execution character set (chosen once per
      compilation).  This may or may not be Unicode.  UCN escapes will
      also be converted from Unicode to the execution character set;
      this happens independent of the source character set.

  <li>Each file referenced by the compiler may state its own character
      set with a <code>#pragma</code>, or rely on the default
      established by the user with locale or a command line option.
      The <code>#pragma</code>, if used, must be the first line in
      the file.  This will not prevent the multiple include
      optimization from working.  GCC will also recognize MULE
      (Multilingual Emacs) magic comments, byte order marks, and any
      other reasonable in-band method of specifying a file's character set.
</ol>

It's worth noting that the standard C library facilities for
"multibyte character sets" are not adequate to implement the above.
The basic problem is that neither C89 nor C99 gives you any way to
specify the character set of a file directly.  You can manipulate the
"locale," which indirectly specifies the character set, but that's a
global change.  Further, locale names are not defined by the C
standard nor is there any consistent map between them and character
sets.

<p>The Single Unix specification, and possibly also POSIX, provide the
<code>nl_langinfo</code> and <code>iconv</code> interfaces which
mostly circumvent these limitations.  There are still difficulties;
for example, cpplib in several places wishes to walk backward through
a string, which is not possible with <code>iconv</code>.  It is,
however, possible.

</body>
</html>
