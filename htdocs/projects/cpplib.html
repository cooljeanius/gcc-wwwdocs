<html>

<head>
<title>cpplib TODO</title>
</head>

<body>
<h1 align="center">Projects relating to cpplib</h1>

<p>As of 6 May 2001, cpplib has largely been completed.  It has
received over one year of testing as the only preprocessor used by
development gcc; it is stable at this point.  It is linked into the C,
C++ and Objective C front ends; this will be the case for GCC 3.0
too.</p>

<h2>Reporting Bugs</h2>

<p>As usual, report bugs with our bug tracking system, GNATS.  Bugs
can be submitted using gccbug (found in the contrib directory) or via
the web interface, and should be put in the "preprocessor" category.
See <a href="../../bugs.html">bugs.html</a> for more details.  A bug
report saying 'package FOO won't compile on system BAR' is useless.
We need short testcases with no system dependencies.  Aim for less
than fifty lines and no #includes at all; we recognize this won't
always be possible.  Please read the rest of this document first!</p>

<p>Also, please file off everything that would cause us legal trouble
if we were to roll your test case into the distributed test suite.
Short test cases will almost always fall under fair use guidelines, so
don't sweat it too much.  An example of a problem is if your test case
includes a 200-line comment detailing inner workings of your program.
(A 200-line comment might be what you need to provoke a bug, but its
contents are unlikely to matter.   Try running it through 
<code>"tr A-Za-z x"</code>.)</p>

<p>Bug reports for code which must be compiled with <code>gcc
-traditional</code> are of interest, but much lower priority than
standard conforming C/C++.  Traditional mode is implemented by a
separate program, not by cpplib.  Oh, and the lack of support for
varargs macros in traditional mode is a deliberate feature.</p>

<h2>Work recently completed</h2>

<ol>
  <li>The dependency generator has been improved, to incorporate all
      the features in <a href="http://gcc.gnu.org/ml/gcc/1999-09n/msg00742.html">
      Tom Tromey's proposal</a> for improving it.

  <li>The identifier hash tables used by cpplib and the front ends
      have been unified, leading to significant memory savings and
      speed gains.
</ol>

<h2>Known Bugs</h2>

<ol>
  <li>Character sets that are <em>not</em> strict supersets of ASCII
      may cause cpplib to mangle the input file, even in comments or
      strings.  Unfortunately, that includes important character sets
      such as Shift JIS and UCS2.  (Please see the discussion of <a
      href="#charset">character set issues</a>, below.)

  <li>Massively parallel builds may cause problems if your system has
      a global limit on the number of files mapped into memory.  I am
      not aware of any system with this problem, it is purely
      theoretical.

  <li>It's reported that on some targets that define their own
      <code>#pragma</code>s, the Fortran and Java compilers fail to
      link; the target-specific code expects a routine called
      <code>c_lex</code> which does not exist in those compilers.
      Possibly affected targets are the c4x, i370, i960, and v850.
</ol>

<h2>Greater Coordination with the Front Ends</h2>

The integrated preprocessor would benefit from greater integration
with the front ends.  It still feels like it has been tacked on as an
after thought, which is not entirely coincidental.

<ol>
  <li>Character sets that are strict supersets of ASCII are safe to
      use, but extended characters cannot appear in identifiers.  This
      has to be coordinated with the C and C++ front ends.  See <a
      href="#charset">character set issues</a>, below.

  <li>C99 universal character escapes (<code>\uxxxx</code>,
      <code>\Uxxxxxxxx</code>) are not recognized in identifiers.
      Proper support has to be coordinated with the front ends.

  <li>Precompiled headers are commonly requested; this entails the
      ability for cpp to dump out and reload all its internal state.
      You can get some of this with the debug switches, but not all,
      and not in a reloadable format.  The front end must cooperate
      also.

  <li>Integration of diagnostic reporting.  The front ends could use
      extra information only available to the preprocessor, such as
      column numbers and macros under expansion.  The existing code
      copies cpplib's internal state into the state used by
      <code>diagnostic.c</code>, which is better than writing out and
      processing linemarker commands, but still suboptimal.

  <li>If YACC did not insist on assigning its own values for token
      codes, there would be no need for a translation layer between
      the codes returned by cpplib and the codes used by the parser.
      Noises have been made about a recursive-descent parser that
      could handle all of C, C++, Objective C; if this ever happens,
      it should use cpplib's token codes.

  <li>String concatenation should be handled in the function
      <code>c_lex</code> in <tt>c-lex.c</tt>.  Then the front ends would
      not have to jump through hoops to remember to concatenate
      strings, and we could simplify the parsers a little too.
</ol>

<h2>Other internal work that needs doing</h2>

<ol>
  <li>We allocate lots of itty bitty items with malloc.  Some work has
      been done on aggregating these into big blocks, using obstacks,
      but we could do even more.  Again, this can be a performance issue.

  <li>VMS support has suffered extreme bit rot.  There may be problems
      with support for DOS, Windows, MVS, and other non-Unixy
      platforms.  No one has complained, though.

  <li>The traditional preprocessor needs a complete overhaul and
      cleanup.  The current code is simply disgusting.  It is chock
      full of buffer overflows, very long functions, and other
      nasties.
</ol>

<h2>Optimizations</h2>

<ol>
  <li>The lexical analyzer and macro expander need to be profiled and
      tuned.  It would be nice to be rid of pool locking - it would
      provide a small speed boost, but this area is quite subtle.  We
      might also benefit from some simple scheme of token lists, both
      for macro expansion and reverting token lookahead.

  <li>It might be worth trying to optimize wrapper headers - files
      containing only an #include of another file, so that they are
      optimized out on reinclusion.  This is more tricky than it may
      sound - something with heuristics similar to the
      multiple-include optimization is needed, but it should also fake
      the include buffer stack properly, and handle multiple levels of
      wrapper headers.
</ol>

<h2><a name="charset">Character set issues</a></h2>

<p>Proper non-ASCII character handling is a hard problem.  Users want
to be able to write comments and strings in their native language.
They want the strings to come out in their native language and not
gibberish after translation to object code.  Some users also want to
use their own alphabet for identifiers in their code.  There is no
one-to-one or many-to-one map between languages and character set
encodings.  The subset of ASCII that is included in most modern day
character sets does not include all the punctuation C uses; some of
the missing punctuation may be present but at a different place than
where it is in ASCII.  The subset described in ISO646 may not be the
smallest subset out there.</p>

<p>At the present time, GCC supports the use of any encoding for
source code, as long as it is a strict superset of 7-bit ASCII.  By
this I mean that all printable (including whitespace) ASCII
characters, when they appear as single bytes in a file, stand only for
themselves, no matter what the context is.  This is true of ISO8859.x,
KOI8-R, and UTF8.  It is not true of Shift JIS and some other popular
Asian character sets.  If they are used, GCC may silently mangle the
input file.  The only known specific example is that a Shift JIS
multibyte character ending with 0x5C will be mistaken for a line
continuation if it occurs at the end of a line.  0x5C is "\" in ASCII.</p>

<p>Assuming a safe encoding, characters not in the base set listed in
the standard (C99 5.2.1) are syntax errors if they appear outside
strings, character constants, or comments.  In strings and character
constants, they are taken literally - converted blindly to numeric
codes, or copied to the assembly output verbatim, depending on the
context.  If you use the C99 <code>\u</code> and <code>\U</code>
escapes, you get UTF8, no exceptions.  These too are only supported in
string and character constants.</p>

<p>We intend to improve this as follows:</p>

<ol>
  <li>cpplib will be reworked so that it can handle any character set
      in wide use, whether or not it is a strict superset of 7-bit
      ASCII.  This means that cpplib will never confuse non-ASCII
      characters with C punctuators, comment delimiters, or whatever.

  <li>In comments, naturally any character will be permitted to appear.

  <li>All Unicode code points which are permitted by C99 Annex D to
      appear in identifiers, will be accepted in identifiers.  All
      source-file characters which, when translated to Unicode,
      correspond to permitted code points, will also be accepted.  In
      assembly output, identifiers will be encoded in UTF8, and then
      reencoded using some mangling scheme if the assembler cannot
      handle UTF8 identifiers.  (Does the new C++ ABI have anything to
      say about this?  What does the Java compiler do?)

      <br>Unicode <code>U+0024</code> will be permitted in
      identifiers if and only if <code>$</code> is permitted.

  <li>In strings and character constants, GCC will translate from the
      character set of the file (selectable on a per-file basis), to
      the current execution character set (chosen once per
      compilation).  This may or may not be Unicode.  UCN escapes will
      also be converted from Unicode to the execution character set;
      this happens independent of the source character set.

  <li>Each file referenced by the compiler may state its own character
      set with a <code>#pragma</code>, or rely on the default
      established by the user with locale or a command line option.
      The <code>#pragma</code>, if used, must be the first line in
      the file.  This will not prevent the multiple include
      optimization from working.  GCC will also recognize MULE
      (Multilingual Emacs) magic comments, byte order marks, and any
      other reasonable in-band method of specifying a file's character set.
</ol>

<p>It's worth noting that the standard C library facilities for
"multibyte character sets" are not adequate to implement the above.
The basic problem is that neither C89 nor C99 gives you any way to
specify the character set of a file directly.  You can manipulate the
"locale," which indirectly specifies the character set, but that's a
global change.  Further, locale names are not defined by the C
standard nor is there any consistent map between them and character
sets.</p>

<p>The Single Unix specification, and possibly also POSIX, provide the
<code>nl_langinfo</code> and <code>iconv</code> interfaces which
mostly circumvent these limitations.  We may require these interfaces
to be present for complete non-ASCII support to be functional.</p>

<p>One final note: EBCDIC is, and will be, supported as a source
character set if and only if GCC is compiled for a host (not a target)
which uses EBCDIC natively.</p>
</body>
</html>
